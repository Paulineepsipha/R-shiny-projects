x = "Text Length Category",
y = "Number of Tweets"
) +
theme_minimal()
# plotly5 <- ggplotly(tweet_len)
# plotly5
tweet_len
custom_breaks <- seq(500, 7000, by = 1000)
p <- tourdefrance_df2 %>%
ggplot(aes(x = Year, y = Distance, text = paste("Year: ", Year, "<br>Distance: ", Distance))) +
geom_point(alpha = 0.5) +
scale_y_continuous(breaks = custom_breaks) +
labs(title = "Evolution of Stage Distance Over Years", x = "Year", y = "Distance") +
theme_minimal() +
theme(legend.position = "none")
# pplotly <- ggplotly(p)
# pplotly
p
##Loading library Packages
library(rvest)
library(ggplot2)
library(gridExtra)
library(plotly)
library(naniar)
library(tidyverse)
library(tidyr)
library(dplyr)
library(RColorBrewer)
library(readr)
library(caret)
library(rpart)
library(rpart.plot)
library(corrplot)
##Scrape from URL
activestock_url <- "https://finance.yahoo.com/most-active"
##Convert from URL to page
activestock_page <- read_html(activestock_url)
##Convert from page to tables
activestock_tables <- html_table(activestock_page, fill = TRUE)
##Choose one table that we are using for analysis
activestock_df <- activestock_tables[[1]]
activestock_df
sorted_data <- activestock_df[order(-activestock_df$`Price (Intraday)`), ]
# Select the top 10 stocks
top_10_stocks <- head(sorted_data, 10)
# Create the customized bar chart
ggplot_chart <- ggplot(top_10_stocks, aes(x = reorder(Name, -`Price (Intraday)`), y = `Price (Intraday)`)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 10 Stocks by Intraday Price", y = "Price (Intraday)") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
scale_y_continuous(labels = scales::dollar_format(scale = 1))
# # Convert the ggplot chart to a Plotly chart
# plotly_chart <- ggplotly(ggplot_chart)
#
# # View the interactive Plotly chart
# plotly_chart
ggplot_chart
sorted_data <- activestock_df[order(-activestock_df$`Price (Intraday)`), ]
# Select the top 10 stocks
top_10_stocks <- head(sorted_data, 10)
# Create the customized bar chart
ggplot_chart <- ggplot(top_10_stocks, aes(x = reorder(Name, -`Price (Intraday)`), y = `Price (Intraday)`)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 10 Stocks by Intraday Price", y = "Price (Intraday)") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
scale_y_continuous(labels = scales::dollar_format(scale = 1))
# Convert the ggplot chart to a Plotly chart
plotly_chart <- ggplotly(ggplot_chart)
# View the interactive Plotly chart
plotly_chart
#
# ggplot_chart
tourdefrance_url <- "https://en.wikipedia.org/wiki/List_of_Tour_de_France_general_classification_winners"
# Scrape ODI data
tourdefrance_page <- read_html(tourdefrance_url)
tourdefrance_table <- html_table(tourdefrance_page, fill = TRUE)
tourdefrance_df <- tourdefrance_table[[3]]
colnames(tourdefrance_df)=c("Year", "Country", "Cyclist","Sponsor/Team", "Distance", "Time/Points", "Margin", "Stage wins" )
# View(tourdefrance_df)
years_to_remove <- c(1915, 1916, 1917, 1918, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1999,2000,2001,2002,2003,2004,2005)
# Filter rows where the Year column is not in the years to remove
tourdefrance_df2 <- tourdefrance_df[!tourdefrance_df$Year %in% years_to_remove, ]
# View(tourdefrance_df2)
##Data wrangling:
tourdefrance_df2$Distance <- gsub("\\(.*\\)", "", tourdefrance_df2$Distance)
tourdefrance_df2$Distance <- gsub("km", "", tourdefrance_df2$Distance)
tourdefrance_df2$Distance <- gsub(",", "", tourdefrance_df2$Distance)
tourdefrance_df2$Distance <- gsub("[^0-9.]", "", tourdefrance_df2$Distance)
tourdefrance_df2$Distance <- as.numeric(tourdefrance_df2$Distance)
str(tourdefrance_df2)
miss_var_summary(tourdefrance_df2)
colnames(tourdefrance_df2)
custom_breaks <- seq(500, 7000, by = 1000)
p <- tourdefrance_df2 %>%
ggplot(aes(x = Year, y = Distance, text = paste("Year: ", Year, "<br>Distance: ", Distance))) +
geom_point(alpha = 0.5) +
scale_y_continuous(breaks = custom_breaks) +
labs(title = "Evolution of Stage Distance Over Years", x = "Year", y = "Distance") +
theme_minimal() +
theme(legend.position = "none")
pplotly <- ggplotly(p)
pplotly
#
# p
country_counts <- tourdefrance_df2 %>%
group_by(Country) %>%
summarize(count = n()) %>%
arrange(desc(count))
top_10_countries <- head(country_counts, 10)
# View(top_10_countries)
# Create the bar plot
top_10_plot <- top_10_countries %>%
ggplot(aes(x = factor(Country, levels = unique(Country)), y = count, fill = Country)) +
geom_bar(stat = "identity") +
labs(fill = "Neighborhood") +
ggtitle("Country dominance in winners") +
theme(legend.position = "right", axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
xlab("Country")
top_10_plotly <- ggplotly(top_10_plot)
top_10_plotly
# top_10_plot
##Reading Csv file:
olymp_tweet_df <- read_csv("Olympics_tweets.csv", show_col_types = FALSE)
head(olymp_tweet_df)
date_format <- "%d/%m/%Y %H:%M"
##Mutating to datatypes referred in question:
olymp_tweet_df1 <- olymp_tweet_df %>%
mutate(
date = as.POSIXct(date, format = date_format),
user_created_at = as.POSIXct(user_created_at, format = date_format),
user_friends = as.integer(user_friends),
retweet_count = as.integer(retweet_count),
favorite_count = as.integer(favorite_count),
user_followers = as.integer(user_followers),
id = as.character(id),
text = as.character(text),
user_screen_name = as.character(user_screen_name),
user_location = as.character(user_location),
user_description = as.character(user_description),
language = as.character(language),
favorited = as.logical(favorited)
)
str(olymp_tweet_df1)
library(lubridate)
olymp_tweet_df1$user_created_at_year <- year(olymp_tweet_df1$user_created_at)
head(olymp_tweet_df1)
distinct_years <- unique(olymp_tweet_df1$user_created_at_year)
print(distinct_years)
miss_var_summary(olymp_tweet_df1)
miss_var_summary(olymp_tweet_df1)
olymp_tweet_df2 <- olymp_tweet_df1 %>%
filter(!is.na(user_created_at_year))
miss_var_summary(olymp_tweet_df2)
user_created_at_counts <- olymp_tweet_df2 %>%
group_by(user_created_at_year) %>%
summarise(count = n())
p1 <- ggplot(user_created_at_counts, aes(x = user_created_at_year, y = count)) +
geom_bar(stat = "identity", fill = "maroon") +
geom_text(aes(label = user_created_at_year), vjust = -0.5, size = 3) +
labs(
title = "Number of Twitter Accounts Created by Year",
x = "Year",
y = "Count"
) +
theme_minimal()
plotly1 <- ggplotly(p1)
plotly1
# p1
summary(user_created_at_counts)
filtered_data <- olymp_tweet_df2 %>%
filter(user_created_at_year > 2010)
# View(filtered_data)
# Calculate the average user_followers for each year
average_user_followers <- filtered_data %>%
group_by(user_created_at_year) %>%
summarise(average_followers = mean(user_followers))
# View(average_user_followers)
# Plot the bar chart
p2 <- ggplot(average_user_followers, aes(x = user_created_at_year, y = average_followers)) +
geom_bar(stat = "identity", fill = "#F1C40F") +
geom_text(aes(label = user_created_at_year), vjust = -0.5, size = 3) +
labs(
title = "Average User Followers by Year (Users Created After 2010)",
x = "Year",
y = "Average User Followers"
) +
theme_minimal()
plotly2 <- ggplotly(p2)
plotly2
# p2
# Calculate the average user_followers for each year
average_user_friends <- filtered_data %>%
group_by(user_created_at_year) %>%
summarise(average_friends = mean(user_friends))
# View(average_user_friends)
# Plot the bar chart
p3 <- ggplot(average_user_friends, aes(x = user_created_at_year, y = average_friends)) +
geom_bar(stat = "identity", fill = "#1E8449") +
geom_text(aes(label = user_created_at_year), vjust = -0.5, size = 3) +
labs(
title = "Average User Friends by Year (Users Created After 2010)",
x = "Year",
y = "Average User Followers"
) +
theme_minimal()
plotly3 <- ggplotly(p3)
plotly3
# p3
loc_filtered_data <- olymp_tweet_df2 %>%
filter(!is.na(user_location))
miss_var_summary(loc_filtered_data)
# Count the occurrences of each location value and select the top 10
top_locations <- loc_filtered_data %>%
count(user_location, sort = TRUE) %>%
head(10) %>%
rename(tweets_count = n)
# Display the top 10 most frequent location values and their tweet counts
print(top_locations)
# Calculate the total number of tweets in the top 10 locations
total_tweets_in_top_10_locations <- sum(top_locations$tweets_count)
# Display the total number of tweets
print(total_tweets_in_top_10_locations)
olymp_tweet_df2 <- olymp_tweet_df2 %>%
mutate(date_extracted = format(as.Date(as.POSIXct(date, format = "%d/%m/%y %H:%M")), "%d/%m/%y"))
head(olymp_tweet_df2)
miss_var_summary(olymp_tweet_df2)
tweets_per_day <- olymp_tweet_df2 %>%
count(date_extracted, sort = TRUE)
# Create a bar chart
tpd <- ggplot(tweets_per_day, aes(x = date_extracted, y = n)) +
geom_bar(stat = "identity", fill = "#D35400") +
labs(
title = "Number of Tweets Posted on Different Dates",
x = "Date",
y = "Number of Tweets"
) +
theme_minimal()
plotly4 <- ggplotly(tpd)
plotly4
# tpd
# Find the date with the lowest number of tweets
date_with_lowest_tweets <- tweets_per_day %>%
slice_min(order_by = n, n = 1) %>%
rename(tweets_count = n)
print(date_with_lowest_tweets)
olymp_tweet_df2 <- olymp_tweet_df2 %>%
mutate(text_length = nchar(text))
# Create a bar chart to show the number of tweets in different text length categories
tweet_length_categories <- olymp_tweet_df2 %>%
mutate(length_category = case_when(
text_length <= 40 ~ "[1, 40]",
text_length <= 80 ~ "[41, 80]",
text_length <= 120 ~ "[81, 120]",
text_length <= 160 ~ "[121, 160]",
text_length <= 200 ~ "[161, 200]",
text_length <= 240 ~ "[201, 240]",
TRUE ~ ">= 241"
)) %>%
count(length_category)
#
# View(tweet_length_categories)
tweet_length_categories <- tweet_length_categories %>%
mutate(length_category = factor(length_category, levels = c("[1, 40]", "[41, 80]", "[81, 120]", "[121, 160]", "[161, 200]", "[201, 240]", ">= 241")))
tweet_len <- ggplot(tweet_length_categories, aes(x = length_category, y = n)) +
geom_bar(stat = "identity", fill = "#6C3483") +
labs(
title = "Number of Tweets in Different Text Length Categories",
x = "Text Length Category",
y = "Number of Tweets"
) +
theme_minimal()
plotly5 <- ggplotly(tweet_len)
plotly5
#
# tweet_len
# Assuming your data frame is named olymp_tweet_df2
tweets_with_mentions <- olymp_tweet_df2 %>%
filter(grepl("@[[:alnum:]_]+", text))
# Count the number of tweets with mentions
num_tweets_with_mentions <- nrow(tweets_with_mentions)
# # Count the number of tweets with at least three different mentions
# num_tweets_with_at_least_three_mentions <- tweets_with_mentions %>%
#   mutate(mentions = str_extract_all(text, "@[[:alnum:]_]+")) %>%
#   filter(length(mentions) >= 3) %>%
#   nrow()
print(paste("Number of tweets with mentions: ", num_tweets_with_mentions))
##Reading the csv file
twitter_data <- read_csv("predictive_twitter_data.csv")
head(twitter_data)
##creating summary for all numerical columns
summary(twitter_data)
##Column names
colnames(twitter_data)
##removing missing values
twitter_data <- twitter_data[complete.cases(twitter_data), ]
##Creating correlation Matrix
correlation_matrix <- cor(twitter_data, method = "spearman")
corrplot(correlation_matrix, method = "color", type = "upper", tl.cex = 0.6, tl.srt = 45, col = colorRampPalette(c("yellow2","goldenrod","darkred"))(100), width = 10, height = 10)
# View(correlation_matrix)
##Sample correlation values:
cor(twitter_data$text_score,twitter_data$text_score_expansion)
##Splitting the data into train_data and test_data
set.seed(1234)  # For reproducibility
splitIndex <- createDataPartition(twitter_data$relevanceJudge, p = 0.7,
list = FALSE,
groups = 1)
training_data <- twitter_data[splitIndex, ]
testing_data <- twitter_data[-splitIndex, ]
dim(training_data)
dim(testing_data)
##fitting the model with train_data
twitter_fit <- rpart(relevanceJudge ~ ., method="class", data = training_data)
##Checking variable importance
var_importance <- tibble(
variable = names(twitter_fit$variable.importance),
importance = twitter_fit$variable.importance)
var_importance %>%
mutate(variable = fct_reorder(variable, importance)) %>%
ggplot(aes(x = importance, y = variable)) +
geom_segment(aes(x = 0, y = variable, xend = importance, yend = variable)) +
geom_point(colour = "red")
printcp(twitter_fit)
##Creating plot
rpart.plot(twitter_fit)
##predicting the test_data
twitter_predict <-predict(twitter_fit, testing_data, type = 'class')
##checking confusion Matrix
table_matrix <- table(testing_data$relevanceJudge, twitter_predict)
table_matrix
##Accuracy test for the predicted value
accuracy_Test <- sum(diag(table_matrix)) / sum(table_matrix)
print(paste('Accuracy for test', accuracy_Test))
twitter_fit_gini <- rpart(relevanceJudge ~ ., data = training_data, parms = list(split = "gini", deviance = "poisson"))
twitter_fit_gini
printcp(twitter_fit_gini)
rpart.plot(twitter_fit_gini)
twitter_fit_entropy <- rpart(relevanceJudge ~ ., data = training_data, parms = list(split = "information"))
rpart.plot(twitter_fit_entropy)
your_threshold <- 0.2
pred_gini <- predict(twitter_fit_gini, newdata = testing_data, type = "vector") >= your_threshold
table(testing_data$relevanceJudge, pred_gini)
your_threshold <- 0.2
pred_entropy <- predict(twitter_fit_entropy, newdata = testing_data, type = "vector") >= your_threshold
table(testing_data$relevanceJudge, pred_entropy)
library(rvest)
library(ggplot2)
library(plotly)
library(dplyr)
library(DAAG)
library(party)
library(rpart)
library(rpart.plot)
library(mlbench)
library(caret)
library(pROC)
library(tree)
library(stringr)
library(dplyr)
library(lubridate)
#Reading the â€œpredictive_twitter_data.csv" file
predictive_twitter_data <- read.csv("predictive_twitter_data.csv")
colnames(predictive_twitter_data)
#Loading the required libraries
library(DAAG)
library(party)
library(rpart)
library(rpart.plot)
library(mlbench)
library(caret)
library(pROC)
library(tree)
str(predictive_twitter_data)
#Reference: https://www.r-bloggers.com/2021/04/decision-trees-in-r/
set.seed(1234)
ind <- sample(2, nrow(predictive_twitter_data), replace = T, prob = c(0.7, 0.3))
train <- predictive_twitter_data[ind == 1,]
test <- predictive_twitter_data[ind == 2,]
#Tree Classification
tree <- rpart(relevanceJudge ~., data = train)
rpart.plot(tree)
printcp(tree)
#Classification tree:
rpart(formula = relevanceJudge ~., data = train)
plotcp(tree)
model1 <- rpart(relevanceJudge ~., data = train, method = 'class')
#Making the predictions on the test data
predictions <- predict(model1, test, type = "class")
#Creating the confusion matrix
confusion_matrix <- table(Actual = test$relevanceJudge, Predicted = predictions)
#Calculating the accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
#Calculating the precison, recall and F1-score
precision <- confusion_matrix[2,2] / sum(confusion_matrix[,2])
recall <- confusion_matrix[2,2] / sum(confusion_matrix[2,])
f1_score <- 2*(precision * recall) / (precision + recall)
#Printing the confusion matrix and evaluation metrics
confusion_matrix
cat("Accuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1_score:", f1_score, "\n")
#removing missing values
p_twitter_data <- predictive_twitter_data[complete.cases(predictive_twitter_data), ]
#Setting the seed
set.seed(7)
#Load the library caret
library(caret)
#Prepare the training data
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
#Train the decision tree model
model <- train(relevanceJudge~., data = p_twitter_data, method = 'rpart', trControl = control)
#Estimating the variable importance
importance <- varImp(model, scale = FALSE)
#Summarizing the importance
print(importance)
#Increasing the firgure size before the plotting
par(mar = c(4, 4, 1, 1))
#Plot of importance
plot(importance)
#Creating a new dataset with a subset of the variables  the important ones
pt_subset <- predictive_twitter_data %>%
select(relevanceJudge,text_score_expansion, text_score, hasURL, semantic_overlap, isReply, tweet_topic_time_diff, X.entities)
#Knowing number of rows in the dataset
nrow(predictive_twitter_data)
#Calcualting the "NA" Values
sum(is.na(predictive_twitter_data))
#https://bookdown.org/rwnahhas/IntroToR/exclude-observations-with-missing-data.html
# Exclude rows that have missing data in ANY variable
predictive_twitter_data_subset <- na.omit(predictive_twitter_data)
nrow(predictive_twitter_data_subset)
library(randomForest)
library(datasets)
library(caret)
# #dim(predictive_twitter_data)
# set.seed(1234)
# ind <- sample(2, nrow(predictive_twitter_data), replace = T, prob = c(0.7, 0.3))
# train <- predictive_twitter_data[ind == 1,]
# test <- predictive_twitter_data[ind == 2,]
# #Loading the required libraries for Random Forest
# library(caTools)
# library(randomForest)
# library(ROSE)
#
# #Splitting data into train and test sets
# set.seed(140)
# split <- sample.split(predictive_twitter_data, SplitRatio = 0.7)
# train_data <- subset(predictive_twitter_data, split = TRUE)
# test_data <- subset(predictive_twitter_data, split = FALSE)
#
# #Handling the imbalanced data
# train_balanced <- ROSE(relevanceJudge~., data = train, seed =123)$data
#
# #Fitting Random Forest to the balanaced train_data dataset
# set.seed(232)
# model2 <- randomForest(
#   x = train_balanced[-5],
#   y = train_balanced$relevanceJudge,
#   ntree = 500
# )
# ```
#
# ```{r}
#
# #Predicting the test set results
# prediction <- predict(model2, new = test[-5])
#
# #Confusion matrix
# confusion_mtx <- table(test[,5], prediction)
#
# #Calculating accuracy
# accuracy <- sum(diag(confusion_mtx)) / sum(confusion_mtx)
#
# #Calculating precision, recall and f1 score
# precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
# recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
# f1_score <- 2 * (precision * recall) / (precision + recall)
#
# # Print the confusion matrix and evaluation metrics
# confusion_matrix
# cat("Accuracy: ", accuracy, "\n")
# cat("Precision: ", precision, "\n")
# cat("Recall: ", recall, "\n")
# cat("F1-Score: ", f1_score, "\n")
library(rvest)
library(ggplot2)
library(plotly)
library(dplyr)
library(DAAG)
library(party)
library(rpart)
library(rpart.plot)
library(mlbench)
library(caret)
library(pROC)
library(tree)
library(stringr)
library(dplyr)
library(lubridate)
ls
ls
?browser
f
f
cont
ls
shiny::runApp('C:/Users/melvi/OneDrive/Desktop/MONASH STUDY/SEMESTER 2/FIT5147-Data exploration and visualisation/Dataviz_Assessment/Programm Exer2/work_file')
library(shiny); runApp('C:/Users/melvi/OneDrive/Desktop/MONASH STUDY/SEMESTER 2/FIT5147-Data exploration and visualisation/Dataviz_Assessment/Data Visualisation Project/DVP_work/Submission/PaulineEpsipha_33665117_Code/PaulineEpsipha_33665117_Code.R')
library(shiny); runApp('C:/Users/melvi/OneDrive/Desktop/MONASH STUDY/SEMESTER 2/FIT5147-Data exploration and visualisation/Dataviz_Assessment/Data Visualisation Project/DVP_work/Submission/PaulineEpsipha_33665117_Code/PaulineEpsipha_33665117_Code.R')
library(shiny); runApp('C:/Users/melvi/OneDrive/Desktop/MONASH STUDY/SEMESTER 2/FIT5147-Data exploration and visualisation/Dataviz_Assessment/Data Visualisation Project/DVP_work/Submission/PaulineEpsipha_33665117_Code/PaulineEpsipha_33665117_Code.R')
library(shiny); runApp('C:/Users/melvi/OneDrive/Desktop/MONASH STUDY/SEMESTER 2/FIT5147-Data exploration and visualisation/Dataviz_Assessment/Data Visualisation Project/DVP_work/Submission/PaulineEpsipha_33665117_Code/PaulineEpsipha_33665117_Code.R')
